{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NaNoGenMo with P.G. Wodehouse \n",
    "\n",
    "### Why this is a terrible idea, but I did it anyway\n",
    "\n",
    "I'd been meaning to participate NaNoGenMo for a while now; I didn't expect that 2020 would be the year that gave me some free time to do so. But there it is.\n",
    "\n",
    "I'd hoped to do something a little more interesting; I'd generally rather be playing with language directly, as I have with my TwitterBots. In  fact, revisiting the damned deployment stuff might have been a better use of my time, because that's what's stopping me from working more on them.\n",
    "\n",
    "I'd also wanted to play around with TensorFlow, having taken at least two extremely boring classes on the subject (so much real estate price regression!). But I had seen several articles about generating text based on particular authors, which was nominally more interesting. And my favorite author is P.G. Wodehouse (hereafter PGW).\n",
    "\n",
    "There are many ironies inherent in choosing P.G. Wodehouse to train a model. He's generally regarded as one of the premiere comic stylists in the English language -- and author with a vocabulary borne English schoolboy reading (and colonialism!), and impeccable sense of comic timing, and a generally sunny worldview. None of these things were going to come through a machine learning algorithm, though, and I knew that going in. \n",
    "\n",
    "The thing about comedy is this: is generally relies on surprise. It's a whole big thing and I'm gnot going to get into it, but I wrote a disseration on this subject a long time ago, so please trust me on this one point.\n",
    "\n",
    "And the other thing is, machine learning relies on probability, i.e. correctly predicting patterns, not even of words, but of characters. The opposite of surprise, really.\n",
    "\n",
    "Ergo, trying to train a model to write funny text is a fool's errand, and a waste of computing resources. Like, I actually felt guilty even wasting other people's CPUs on this task. I knew that, without a doubt, the result would be mediocre at best, just as one would expect from an algorithm that generally thinks 80% is a good score.\n",
    "\n",
    "But in the spirit of 2020, Camus, and Sisyphus, I did it anyway. I tried to pretend I was a tech utopianist and had hope that something good would come of this exercise, or at the very least, that even if it was knowably futile from the outset, it was still somehow worth doing, over and over and over again, like either a machine learning model or Sisyphus or both.\n",
    "\n",
    "The thing is, it was more of a chore than anything else. A way to show that yes, I could use this sort of code, if anyone wanted to know.\n",
    "\n",
    "And also to demonstrate why it was a terrible idea.\n",
    "\n",
    "### Technical details\n",
    "\n",
    "First I scraped all of PGW's available writings from Gutenberg org. I wrote each work into a separate text file because I don't like to rely on internet access and I wasn't sure how much munging I'd have to do.\n",
    "\n",
    "Then I combined P.G. Wodehouse texts that were available on Gutenberg.org to create a training set. Before combining them, I cleaned them as best I could, to remove any text that was written by Gutenberg people and not PGW.\n",
    "\n",
    "Then I followed this (TensorFlow tutorial)![#https://www.tensorflow.org/tutorials/text/text_generation#attach_an_optimizer_and_a_loss_function] that showed how to use Tensorflow to generate text. It was relatively reasonable, I thought, and had some good explanations.\n",
    "\n",
    "It also gave a good TL;DR of how machine learning works\n",
    "\n",
    "Now, training the model takes a lot of time and CPU. One way to better the model, according to the article, is to let it train for more \"epochs\" (let's just define this as imaginary units of time that the machine overlords use). It was already quite a pain in the ass to train for 10 epochs on my own laptop, so I figured I would at attempt to use the magic of cloud computing to make my life easier.\n",
    "\n",
    "It didn't take to long to set up notebooks on Microsoft Azure and Google's Compute Engine; I think it took under an hour. But the training wasn't as reliable as I'd wanted, and it kept hanging. Nevertheless, I persevered. I wanted to generate at least a few \"well-trained\" novels out.\n",
    "\n",
    "\n",
    "\n",
    "### Conclusion\n",
    "The novels generate by machine are exactly as mediocre as one might expect. A better exercise for any aspiring writing is to try to emulate the author's style by hand.  In fact, a better exercise for anyone is try to sort through what patterns the machine is trying to recognize and ask themselves if they want an algorithm making those decisions.\n",
    "\n",
    "I don't even think the texts are as entertaining as some of the other authors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
